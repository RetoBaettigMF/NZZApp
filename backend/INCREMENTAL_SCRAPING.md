# Inkrementelles Scraping - Implementation Complete âœ“

## Was wurde implementiert?

Das NZZ Scraper System wurde um **inkrementelles Scraping mit Artikel-Tracking** erweitert. Der Scraper kann jetzt mehrmals tÃ¤glich ausgefÃ¼hrt werden und scrapt nur neue, noch nicht heruntergeladene Artikel.

## Hauptmerkmale

### 1. Zentrale Tracking-Liste
**Datei:** `articles/scraped_articles.json`

Format:
```json
{
  "articles": [
    {
      "url": "https://www.nzz.ch/...",
      "scraped_date": "2026-02-17",
      "filename": "2026-02-17/kategorie/Artikel_Titel.md",
      "title": "Artikel Titel"
    }
  ],
  "last_updated": "2026-02-17T18:02:48.395846"
}
```

### 2. Neue Scraper-Methoden

**In `scraper.py`:**
- `load_tracked_articles()` - LÃ¤dt persistente Tracking-Datei
- `save_tracked_articles()` - Speichert aktualisierte Tracking-Datei
- `is_article_scraped()` - PrÃ¼ft ob URL bereits gescrapt wurde
- `add_to_tracking()` - FÃ¼gt gescrapten Artikel zum Tracking hinzu
- `update_manifest()` - Aktualisiert Manifest mit allen Artikeln im Ordner

### 3. Ãœberarbeitete `run()` Logik

**Workflow:**
1. Tracking-Datei laden
2. Artikel-Links von Homepage holen
3. **Nur neue Links filtern** (nicht im Tracking)
4. Nur neue Artikel scrapen
5. Jeder Artikel wird sofort zum Tracking hinzugefÃ¼gt
6. Tracking-Datei speichern
7. ZIP des heutigen Tages neu erstellen (Ã¼berschreibt altes)
8. Manifest aktualisieren

### 4. Migration fÃ¼r bestehende Artikel

**Script:** `migrate_tracking.py`

Durchsucht alle bestehenden Artikel-Verzeichnisse und initialisiert die Tracking-Datei mit allen URLs, die bereits heruntergeladen wurden.

## Test-Ergebnisse

### Migration
```
âœ“ 43 Artikel aus bestehenden Ordnern zum Tracking hinzugefÃ¼gt
  - 2026-02-14: 0 Artikel (alte Formatierung)
  - 2026-02-16: 28 Artikel
  - 2026-02-17: 15 Artikel (4 Duplikate erkannt)
```

### Erster Scraper-Run
```
â„¹ 43 Artikel bereits gescrapt
âœ“ 19 Links gefunden auf Homepage
âœ“ 12 NEUE Artikel gescrapt (7 waren bereits bekannt)
âœ“ Tracking aktualisiert: 55 Artikel total
```

### Zweiter Scraper-Run (2 Minuten spÃ¤ter)
```
â„¹ 55 Artikel bereits gescrapt
âœ“ 19 Links gefunden auf Homepage
âœ“ 1 NEUER Artikel gescrapt (18 waren bereits bekannt)
âœ“ Tracking aktualisiert: 56 Artikel total
```

**Beweis:** Ein neuer Artikel erschien zwischen den Runs, und der Scraper erkannte ihn korrekt als neu!

## Vorteile

### âœ… Effizienz
- Nur neue Artikel werden gescrapt
- Spart AI-Bereinigungskosten (OpenRouter)
- Spart Bandbreite und Zeit

### âœ… Inkrementelle Updates
- Scraper kann stÃ¼ndlich laufen (z.B. per Cronjob)
- Holt kontinuierlich neue Artikel Ã¼ber den Tag verteilt
- Keine Duplikate mehr

### âœ… State-Management
- Persistente Tracking-Datei als Single Source of Truth
- Nachvollziehbar: Wann wurde welcher Artikel gescrapt?
- ErmÃ¶glicht Analysen und Statistiken

### âœ… Tages-Organisation
- Artikel werden weiterhin nach Datum organisiert
- Heutiges ZIP wird bei jedem Run aktualisiert
- Alte ZIPs bleiben final und unverÃ¤ndert

## Verzeichnisstruktur

```
articles/
â”œâ”€â”€ scraped_articles.json          # ZENTRALE TRACKING-LISTE
â”œâ”€â”€ 2026-02-14/
â”‚   â”œâ”€â”€ kategorie1/*.md
â”‚   â”œâ”€â”€ kategorie2/*.md
â”‚   â””â”€â”€ manifest.json
â”œâ”€â”€ 2026-02-14.zip                 # FINAL (wird nicht mehr geÃ¤ndert)
â”œâ”€â”€ 2026-02-17/
â”‚   â”œâ”€â”€ wirtschaft/*.md
â”‚   â”œâ”€â”€ sport/*.md
â”‚   â”œâ”€â”€ lokal/*.md
â”‚   â”œâ”€â”€ welt/*.md
â”‚   â””â”€â”€ manifest.json
â””â”€â”€ 2026-02-17.zip                 # AKTUELL (wird bei jedem Run Ã¼berschrieben)
```

## Verwendung

### Normaler Scraper-Run
```bash
cd /home/reto/Development/NZZApp/backend
source venv/bin/activate
python scraper.py
```

**Ausgabe:**
```
â„¹ 56 Artikel bereits gescrapt
â„¹ 19 Links gefunden, 0 sind NEU
âœ“ Keine neuen Artikel zum Scrapen
```

### Migration bestehender Artikel
```bash
python migrate_tracking.py
```

**Nur einmal ausfÃ¼hren!** Initialisiert Tracking-Datei aus bestehenden Artikeln.

## Cronjob-Setup

Um den Scraper mehrmals tÃ¤glich automatisch laufen zu lassen:

```bash
crontab -e
```

Beispiel: Alle 2 Stunden scrapen
```cron
0 */2 * * * cd /home/reto/Development/NZZApp/backend && source venv/bin/activate && python scraper.py >> /tmp/nzz_scraper.log 2>&1
```

## Monitoring

### Tracking-Status prÃ¼fen
```bash
cat articles/scraped_articles.json | jq '{ total: (.articles | length), last_updated: .last_updated }'
```

### Heutiges Manifest prÃ¼fen
```bash
cat articles/$(date +%Y-%m-%d)/manifest.json | jq '.'
```

### Artikel-Count pro Tag
```bash
for dir in articles/2026-*; do
  if [ -d "$dir" ]; then
    count=$(find "$dir" -name "*.md" | wc -l)
    echo "$(basename $dir): $count Artikel"
  fi
done
```

## Technische Details

### Duplikat-Erkennung
- Basiert auf **URL-Matching** (nicht Titel oder Inhalt)
- URLs werden in einem Set gespeichert fÃ¼r O(1) Lookup
- Sehr schnell auch bei tausenden Artikeln

### Manifest-Update
- ZÃ¤hlt **alle** .md Dateien im Tages-Ordner
- Nicht nur neu gescrapte Artikel
- Reflektiert den vollstÃ¤ndigen Ordnerinhalt

### ZIP-Handling
- `zipfile.ZipFile(..., 'w', ...)` Ã¼berschreibt automatisch
- EnthÃ¤lt immer den aktuellen Stand des Tages-Ordners
- Alte ZIPs werden nicht angefasst

### Robustheit
- Tracking-Datei wird nach jedem Artikel-Scraping aktualisiert
- Bei Abbruch: Bereits gescrapte Artikel sind im Tracking
- NÃ¤chster Run Ã¼berspringt sie automatisch

## Potenzielle Erweiterungen

### 1. Artikel-Alterung
```python
# LÃ¶sche Tracking-EintrÃ¤ge Ã¤lter als 90 Tage
cutoff_date = datetime.now() - timedelta(days=90)
tracking['articles'] = [
    a for a in tracking['articles']
    if datetime.fromisoformat(a['scraped_date']) > cutoff_date
]
```

### 2. Statistiken
```python
# Artikel pro Kategorie (gesamt)
from collections import Counter
stats = Counter([a['filename'].split('/')[1] for a in tracking['articles']])
```

### 3. Re-Scraping
```python
# Artikel Ã¤lter als X Tage neu scrapen (fÃ¼r Updates)
def should_rescrape(article, days=30):
    age = datetime.now() - datetime.fromisoformat(article['scraped_date'])
    return age.days > days
```

## Zusammenfassung

Das inkrementelle Scraping-System ist **vollstÃ¤ndig implementiert und getestet**. Es funktioniert zuverlÃ¤ssig und erfÃ¼llt alle Anforderungen:

- âœ… Zentrale Tracking-Liste
- âœ… Nur neue Artikel scrapen
- âœ… Tages-Verzeichnisse beibehalten
- âœ… Heutiges ZIP Ã¼berschreiben
- âœ… Alte ZIPs unverÃ¤ndert

**Status:** Production-ready! ğŸš€
